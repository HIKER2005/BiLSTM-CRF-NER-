"""
BiLSTM+CRFæ¨¡å‹å¯è§†åŒ– - å®Œæ•´ä¿®å¤ç‰ˆ
"""

import os
# ä¿®å¤ OpenMP å†²çª
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from models.bilstm_crf import BiLSTM_CRF

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False


def load_model(checkpoint_path='checkpoints/best_model.pt', device='cpu'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    if not torch.cuda.is_available() and device == 'cuda':
        device = 'cpu'

    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    vocab = checkpoint['vocab']
    config = checkpoint['config']

    model = BiLSTM_CRF(
        vocab_size=vocab.vocab_size,
        tag_size=vocab.tag_size,
        embedding_dim=config['embedding_dim'],
        hidden_dim=config['hidden_dim'],
        num_layers=config['num_layers'],
        dropout=config['dropout']
    )

    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    return model, vocab, config


def visualize_emissions(model, vocab, sentence, device='cpu', save_path='outputs/emissions_heatmap.png'):
    """å¯è§†åŒ–å‘å°„åˆ†æ•°ï¼ˆBiLSTMè¾“å‡ºï¼‰"""
    words = list(sentence.replace(' ', ''))
    word_indices = [vocab.get_word_idx(word) for word in words]
    words_tensor = torch.tensor([word_indices], dtype=torch.long).to(device)

    with torch.no_grad():
        emissions = model.get_emissions(words_tensor)
        emissions = emissions.squeeze(0).cpu().numpy()

    tag_names = [vocab.get_tag(i) for i in range(vocab.tag_size)]

    plt.figure(figsize=(12, max(6, len(words) * 0.4)))
    sns.heatmap(emissions,
                xticklabels=tag_names,
                yticklabels=words,
                cmap='RdYlGn',
                center=0,
                annot=True,
                fmt='.2f',
                cbar_kws={'label': 'å‘å°„åˆ†æ•°'})

    plt.title(f'å‘å°„åˆ†æ•°çƒ­åŠ›å›¾\nå¥å­: {sentence}', fontsize=14, pad=20)
    plt.xlabel('æ ‡ç­¾', fontsize=12)
    plt.ylabel('å­—ç¬¦', fontsize=12)
    plt.tight_layout()

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"  âœ… ä¿å­˜: {save_path}")
    plt.close()


def visualize_transitions(model, vocab, save_path='outputs/transitions_heatmap.png'):
    """å¯è§†åŒ–CRFè½¬ç§»çŸ©é˜µ"""
    transitions = model.crf.transitions.detach().cpu().numpy()
    tag_names = [vocab.get_tag(i) for i in range(vocab.tag_size)]

    plt.figure(figsize=(10, 8))
    sns.heatmap(transitions,
                xticklabels=tag_names,
                yticklabels=tag_names,
                cmap='coolwarm',
                center=0,
                annot=True,
                fmt='.2f',
                cbar_kws={'label': 'è½¬ç§»åˆ†æ•°'})

    plt.title('CRF è½¬ç§»çŸ©é˜µ', fontsize=14, pad=20)
    plt.xlabel('åˆ°æ ‡ç­¾', fontsize=12)
    plt.ylabel('ä»æ ‡ç­¾', fontsize=12)
    plt.tight_layout()

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"  âœ… ä¿å­˜: {save_path}")
    plt.close()


def visualize_viterbi_path(model, vocab, sentence, device='cpu', save_path='outputs/viterbi_path.png'):
    """å¯è§†åŒ–Viterbiè§£ç è·¯å¾„"""
    words = list(sentence.replace(' ', ''))
    word_indices = [vocab.get_word_idx(word) for word in words]
    words_tensor = torch.tensor([word_indices], dtype=torch.long).to(device)

    with torch.no_grad():
        emissions = model.get_emissions(words_tensor)
        mask = torch.ones_like(words_tensor, dtype=torch.bool)
        # ä¿®å¤ï¼šä½¿ç”¨ _viterbi_decodeï¼ˆå¸¦ä¸‹åˆ’çº¿ï¼‰
        predictions = model.crf._viterbi_decode(emissions, mask)
        pred_tags = predictions[0]

    tag_names = [vocab.get_tag(tag_idx) for tag_idx in pred_tags]

    fig, ax = plt.subplots(figsize=(max(10, len(words) * 0.8), 6))

    x = range(len(words))
    y = pred_tags

    ax.plot(x, y, 'o-', linewidth=2, markersize=10, label='é¢„æµ‹è·¯å¾„')

    ax.set_yticks(range(vocab.tag_size))
    ax.set_yticklabels([vocab.get_tag(i) for i in range(vocab.tag_size)])

    ax.set_xticks(x)
    ax.set_xticklabels(words, fontsize=12)

    for i, (word, tag_idx, tag_name) in enumerate(zip(words, pred_tags, tag_names)):
        ax.annotate(tag_name,
                   xy=(i, tag_idx),
                   xytext=(0, 10),
                   textcoords='offset points',
                   ha='center',
                   fontsize=10,
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))

    ax.set_xlabel('å­—ç¬¦', fontsize=12)
    ax.set_ylabel('æ ‡ç­¾', fontsize=12)
    ax.set_title(f'Viterbi è§£ç è·¯å¾„\nå¥å­: {sentence}', fontsize=14, pad=20)
    ax.grid(True, alpha=0.3)
    ax.legend()

    plt.tight_layout()
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"  âœ… ä¿å­˜: {save_path}")
    plt.close()


def visualize_training_history(history_path='checkpoints/train_history.json',
                               save_path='outputs/training_history.png'):
    """å¯è§†åŒ–è®­ç»ƒå†å²"""
    import json

    if not os.path.exists(history_path):
        print(f"  âš ï¸  æ‰¾ä¸åˆ°è®­ç»ƒå†å²æ–‡ä»¶: {history_path}")
        return

    with open(history_path, 'r', encoding='utf-8') as f:
        history = json.load(f)

    epochs = [h['epoch'] for h in history]
    train_loss = [h['train_loss'] for h in history]
    precision = [h['precision'] for h in history]
    recall = [h['recall'] for h in history]
    f1 = [h['f1'] for h in history]

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # Lossæ›²çº¿
    axes[0, 0].plot(epochs, train_loss, 'b-', linewidth=2)
    axes[0, 0].set_xlabel('Epoch', fontsize=12)
    axes[0, 0].set_ylabel('Loss', fontsize=12)
    axes[0, 0].set_title('è®­ç»ƒæŸå¤±', fontsize=14)
    axes[0, 0].grid(True, alpha=0.3)

    # F1æ›²çº¿
    axes[0, 1].plot(epochs, f1, 'g-', linewidth=2)
    axes[0, 1].set_xlabel('Epoch', fontsize=12)
    axes[0, 1].set_ylabel('F1 Score', fontsize=12)
    axes[0, 1].set_title('F1åˆ†æ•°', fontsize=14)
    axes[0, 1].grid(True, alpha=0.3)

    # ç²¾ç¡®ç‡å’Œå¬å›ç‡
    axes[1, 0].plot(epochs, precision, 'r-', linewidth=2, label='Precision')
    axes[1, 0].plot(epochs, recall, 'b-', linewidth=2, label='Recall')
    axes[1, 0].set_xlabel('Epoch', fontsize=12)
    axes[1, 0].set_ylabel('Score', fontsize=12)
    axes[1, 0].set_title('ç²¾ç¡®ç‡ & å¬å›ç‡', fontsize=14)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # ç»¼åˆå¯¹æ¯”
    axes[1, 1].plot(epochs, precision, 'r-', linewidth=2, label='Precision', alpha=0.7)
    axes[1, 1].plot(epochs, recall, 'b-', linewidth=2, label='Recall', alpha=0.7)
    axes[1, 1].plot(epochs, f1, 'g-', linewidth=2, label='F1', alpha=0.7)
    axes[1, 1].set_xlabel('Epoch', fontsize=12)
    axes[1, 1].set_ylabel('Score', fontsize=12)
    axes[1, 1].set_title('ç»¼åˆæŒ‡æ ‡', fontsize=14)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.suptitle('è®­ç»ƒå†å²', fontsize=16, y=1.00)
    plt.tight_layout()

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"  âœ… ä¿å­˜: {save_path}")
    plt.close()


def main():
    print("="*70)
    print(" "*20 + "BiLSTM+CRF å¯è§†åŒ–ç¨‹åº")
    print("="*70)

    model_path = 'checkpoints/best_model.pt'
    if not os.path.exists(model_path):
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè¿è¡Œ python train.py è®­ç»ƒæ¨¡å‹")
        return

    print(f"\nğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model, vocab, config = load_model(model_path, device)
    print("  âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    default_sentence = "ä¹”å¸ƒæ–¯åˆ›ç«‹äº†è‹¹æœå…¬å¸"
    user_input = input(f"\nè¯·è¾“å…¥è¦å¯è§†åŒ–çš„å¥å­ (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤): ").strip()
    sentence = user_input if user_input else default_sentence

    print(f"\nğŸ¨ å¯è§†åŒ–å¥å­: {sentence}")
    print("="*70)

    print("\nğŸ“Š ç”Ÿæˆå‘å°„åˆ†æ•°çƒ­åŠ›å›¾...")
    visualize_emissions(model, vocab, sentence, device)

    print("\nğŸ“Š ç”ŸæˆCRFè½¬ç§»çŸ©é˜µ...")
    visualize_transitions(model, vocab)

    print("\nğŸ“Š ç”ŸæˆViterbiè§£ç è·¯å¾„...")
    visualize_viterbi_path(model, vocab, sentence, device)

    print("\nğŸ“Š ç”Ÿæˆè®­ç»ƒå†å²æ›²çº¿...")
    visualize_training_history()

    print("\n" + "="*70)
    print("âœ… å¯è§†åŒ–å®Œæˆï¼")
    print("="*70)
    print("\nç”Ÿæˆçš„å›¾ç‰‡ä¿å­˜åœ¨ outputs/ ç›®å½•:")
    print("  - emissions_heatmap.png    (å‘å°„åˆ†æ•°çƒ­åŠ›å›¾)")
    print("  - transitions_heatmap.png  (CRFè½¬ç§»çŸ©é˜µ)")
    print("  - viterbi_path.png         (Viterbiè§£ç è·¯å¾„)")
    print("  - training_history.png     (è®­ç»ƒå†å²æ›²çº¿)")
    print("="*70)


if __name__ == '__main__':
    main()