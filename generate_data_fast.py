"""
DeepSeek NER å¿«é€Ÿæ•°æ®ç”Ÿæˆå™¨ - ä¸¥æ ¼çº¦æŸç‰ˆæœ¬
å¸¦å®Œæ•´éªŒè¯ã€è‡ªåŠ¨ä¿®å¤å’Œå¹¶è¡ŒåŠ é€Ÿ
"""

import os
import requests
import json
import time
from tqdm import tqdm
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter
import random


class StrictDeepSeekDataGenerator:
    def __init__(self, api_key, base_url="https://api.deepseek.com/v1"):
        self.api_key = api_key
        self.base_url = base_url
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_generated': 0,
            'format_fixed': 0,
            'boundary_fixed': 0,
            'invalid_samples': 0,
            'api_errors': 0
        }

    def call_api(self, messages, temperature=0.7, max_retries=3):
        """è°ƒç”¨API withé‡è¯•"""
        url = f"{self.base_url}/chat/completions"

        for attempt in range(max_retries):
            try:
                payload = {
                    "model": "deepseek-chat",
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": 2000
                }

                response = requests.post(
                    url,
                    headers=self.headers,
                    json=payload,
                    timeout=30
                )
                response.raise_for_status()
                return response.json()['choices'][0]['message']['content']

            except Exception as e:
                if attempt == max_retries - 1:
                    self.stats['api_errors'] += 1
                    print(f"\n  âŒ APIè°ƒç”¨å¤±è´¥: {e}")
                    return None
                time.sleep(1)

        return None

    def get_strict_system_prompt(self):
        """è·å–ä¸¥æ ¼çš„ç³»ç»Ÿæç¤º"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„NERæ•°æ®æ ‡æ³¨ä¸“å®¶ã€‚

    **ç¡¬æ€§è§„åˆ™**ï¼š
    1. åªèƒ½ä½¿ç”¨è¿™7ç§æ ‡ç­¾ï¼šB-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, O
    2. ç»å¯¹ç¦æ­¢ï¼šPER-B, M-ORG, E-ORG, S-PER, PER, LOC, ORG
    3. æ¯ä¸ªå®ä½“ç¬¬ä¸€ä¸ªå­—å¿…é¡»æ˜¯B-ï¼Œåç»­å­—å¿…é¡»æ˜¯I-
    4. è¾“å‡ºæ ¼å¼ï¼šå­—<ç©ºæ ¼>æ ‡ç­¾ï¼Œæ¯è¡Œä¸€ä¸ª
    
    è¿åä»»ä½•è§„åˆ™éƒ½æ˜¯é”™è¯¯ã€‚"""

    def generate_sentences(self, num_sentences=50):
        """ç”Ÿæˆå¥å­"""
        prompt = f"""ç”Ÿæˆ{num_sentences}ä¸ªä¸­æ–‡å¥å­ç”¨äºNERæ ‡æ³¨ã€‚

        è¦æ±‚ï¼š
        1. å¥å­é•¿åº¦ï¼š8-30å­—
        2. å¿…é¡»åŒ…å«è‡³å°‘1ä¸ªå‘½åå®ä½“
        3. å®ä½“ç±»å‹è¦å‡è¡¡ï¼šäººå(PER)ã€åœ°å(LOC)ã€æœºæ„å(ORG)
        4. å¥å­è‡ªç„¶ã€çœŸå®ã€è¯­æ³•æ­£ç¡®
        
        å®ä½“ç¤ºä¾‹ï¼š
        - äººå(PER): é©¬äº‘ã€å§šæ˜ã€é²è¿…ã€å‘¨æ°ä¼¦ã€é’Ÿå—å±±
        - åœ°å(LOC): åŒ—äº¬ã€ä¸Šæµ·ã€é•¿æ±Ÿã€é»„å±±ã€ä¸­å›½
        - æœºæ„(ORG): é˜¿é‡Œå·´å·´ã€æ¸…åå¤§å­¦ã€ä¸­å›½é“¶è¡Œã€è”åˆå›½
        
        åªè¾“å‡ºå¥å­ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦ç¼–å·ã€‚
        
        ç”Ÿæˆ{num_sentences}ä¸ªå¥å­ï¼š"""

        messages = [
            {"role": "system", "content": self.get_strict_system_prompt()},
            {"role": "user", "content": prompt}
        ]

        response = self.call_api(messages, temperature=0.9)

        if response:
            sentences = []
            for line in response.strip().split('\n'):
                # æ¸…ç†ç¼–å·
                line = re.sub(r'^\d+[\.\ã€\s]+', '', line.strip())
                # æ¸…ç†å¼•å·
                line = line.strip('"\'""''')

                if line and 5 <= len(line) <= 100:
                    sentences.append(line)

            return sentences
        return []

    def annotate_sentence(self, sentence):
        """æ ‡æ³¨å•ä¸ªå¥å­ - å¸¦ä¸¥æ ¼çº¦æŸ"""
        prompt = f"""å¯¹å¥å­è¿›è¡ŒBIOæ ‡æ³¨ã€‚

        **ä¸¥æ ¼è¦æ±‚**ï¼š
        1. åªèƒ½ä½¿ç”¨è¿™7ç§æ ‡ç­¾ï¼šB-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, O
        2. å®ä½“ç¬¬ä¸€ä¸ªå­—ç”¨B-ï¼Œåç»­å­—ç”¨I-
        3. æ ¼å¼ï¼šå­—<ç©ºæ ¼>æ ‡ç­¾ï¼Œæ¯è¡Œä¸€ä¸ª
        
        **ç¦æ­¢çš„æ ‡ç­¾**ï¼š
        âŒ PER-B, LOC-I (å€’åº)
        âŒ M-ORG, E-ORG (BMES)
        âŒ PER, LOC, ORG (æ— å‰ç¼€)
        
        å¥å­ï¼š{sentence}
        
        ä¸¥æ ¼æŒ‰ç…§BIOæ ¼å¼æ ‡æ³¨ï¼š"""

        messages = [
            {"role": "system", "content": self.get_strict_system_prompt()},
            {"role": "user", "content": prompt}
        ]

        response = self.call_api(messages, temperature=0.3)

        if response:
            try:
                # è§£ææ ‡æ³¨
                words, tags = self.parse_annotation(response)

                # éªŒè¯é•¿åº¦
                if len(words) != len(sentence.replace(' ', '')):
                    return None

                # æ¸…æ´—å’ŒéªŒè¯
                words, tags = self.clean_sample(words, tags)

                # æœ€ç»ˆéªŒè¯
                is_valid, msg = self.validate_sample(words, tags)

                if is_valid:
                    self.stats['total_generated'] += 1
                    return words, tags
                else:
                    self.stats['invalid_samples'] += 1

            except Exception as e:
                self.stats['invalid_samples'] += 1

        return None

    def parse_annotation(self, response):
        """è§£ææ ‡æ³¨å“åº”"""
        words = []
        tags = []

        for line in response.strip().split('\n'):
            line = line.strip()

            # è·³è¿‡ç©ºè¡Œå’Œmarkdown
            if not line or line.startswith('```') or line.startswith('#'):
                continue

            # è§£æ "å­— æ ‡ç­¾"
            parts = line.split()
            if len(parts) == 2:
                word, tag = parts
                if len(word) == 1:  # åªè¦å•ä¸ªå­—ç¬¦
                    words.append(word)
                    tags.append(tag)

        return words, tags

    def fix_tag_format(self, tag):
        """ä¿®å¤æ ‡ç­¾æ ¼å¼"""
        tag = tag.strip().upper()

        # å·²ç»æ­£ç¡®
        if tag in ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'O']:
            return tag

        # å€’åº: PER-B â†’ B-PER
        if re.match(r'^(PER|LOC|ORG)-([BI])$', tag):
            entity_type, prefix = tag.split('-')
            self.stats['format_fixed'] += 1
            return f'{prefix}-{entity_type}'

        # æ— å‰ç¼€: PER â†’ B-PER
        if tag in ['PER', 'LOC', 'ORG']:
            self.stats['format_fixed'] += 1
            return f'B-{tag}'

        # BMESæ ¼å¼: M-ORG â†’ I-ORG
        if re.match(r'^[BMES]-(PER|LOC|ORG)$', tag):
            prefix, entity_type = tag.split('-')
            self.stats['format_fixed'] += 1
            if prefix == 'B':
                return f'B-{entity_type}'
            elif prefix in ['M', 'E', 'I']:
                return f'I-{entity_type}'
            elif prefix == 'S':
                return f'B-{entity_type}'

        # è‹±æ–‡å…¨ç§°
        tag_mapping = {
            'PERSON': 'PER', 'PEOPLE': 'PER',
            'LOCATION': 'LOC', 'PLACE': 'LOC',
            'ORGANIZATION': 'ORG', 'COMPANY': 'ORG'
        }

        for old, new in tag_mapping.items():
            if old in tag:
                self.stats['format_fixed'] += 1
                if 'B' in tag or tag == old:
                    return f'B-{new}'
                else:
                    return f'I-{new}'

        # æ— æ³•è¯†åˆ«
        return 'O'

    def fix_entity_boundaries(self, words, tags):
        """ä¿®å¤å®ä½“è¾¹ç•Œ"""
        fixed_tags = []

        for i, tag in enumerate(tags):
            # I- æ ‡ç­¾æ£€æŸ¥
            if tag.startswith('I-'):
                entity_type = tag[2:]
                # å‰é¢å¿…é¡»æ˜¯åŒç±»å‹çš„ B- æˆ– I-
                if i == 0 or not fixed_tags[-1].endswith(f'-{entity_type}'):
                    fixed_tags.append(f'B-{entity_type}')
                    self.stats['boundary_fixed'] += 1
                else:
                    fixed_tags.append(tag)
            else:
                fixed_tags.append(tag)

        return fixed_tags

    def clean_sample(self, words, tags):
        """æ¸…æ´—æ ·æœ¬"""
        # 1. ä¿®å¤æ ‡ç­¾æ ¼å¼
        fixed_tags = [self.fix_tag_format(tag) for tag in tags]

        # 2. ä¿®å¤å®ä½“è¾¹ç•Œ
        fixed_tags = self.fix_entity_boundaries(words, fixed_tags)

        return words, fixed_tags

    def validate_sample(self, words, tags):
        """éªŒè¯æ ·æœ¬æœ‰æ•ˆæ€§"""
        # é•¿åº¦æ£€æŸ¥
        if len(words) != len(tags):
            return False, "é•¿åº¦ä¸åŒ¹é…"

        if len(words) < 5:
            return False, "å¥å­å¤ªçŸ­"

        if len(words) > 100:
            return False, "å¥å­å¤ªé•¿"

        # æ ‡ç­¾åˆæ³•æ€§
        valid_tags = {'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'O'}
        for tag in tags:
            if tag not in valid_tags:
                return False, f"éæ³•æ ‡ç­¾: {tag}"

        # è‡³å°‘æœ‰ä¸€ä¸ªå®ä½“
        has_entity = any(tag.startswith('B-') for tag in tags)
        if not has_entity:
            return False, "æ— å®ä½“"

        # è¾¹ç•Œä¸€è‡´æ€§
        for i, tag in enumerate(tags):
            if tag.startswith('I-'):
                entity_type = tag[2:]
                if i == 0 or not tags[i-1].endswith(f'-{entity_type}'):
                    return False, "è¾¹ç•Œé”™è¯¯"

        return True, "OK"

    def annotate_batch_parallel(self, sentences, max_workers=5):
        """å¹¶è¡Œæ ‡æ³¨"""
        results = [None] * len(sentences)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_idx = {
                executor.submit(self.annotate_sentence, sent): idx
                for idx, sent in enumerate(sentences)
            }

            with tqdm(total=len(sentences), desc="  æ ‡æ³¨è¿›åº¦", ncols=80) as pbar:
                for future in as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        result = future.result()
                        results[idx] = result
                    except Exception as e:
                        pass
                    pbar.update(1)

        return results

    def format_sample(self, words, tags):
        """æ ¼å¼åŒ–æ ·æœ¬ä¸ºå­—ç¬¦ä¸²"""
        lines = []
        for word, tag in zip(words, tags):
            lines.append(f'{word} {tag}')
        return '\n'.join(lines)

    def generate_dataset(self, num_sentences=200, output_file='data/generated_data.txt',
                         batch_size=50, max_workers=5):
        """ç”Ÿæˆæ•°æ®é›†"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        all_samples = []
        seen_sentences = set()
        num_batches = (num_sentences + batch_size - 1) // batch_size

        print(f"\n{'='*70}")
        print(f"{'DeepSeek NER ä¸¥æ ¼æ•°æ®ç”Ÿæˆå™¨':^70}")
        print(f"{'='*70}")
        print(f"\né…ç½®:")
        print(f"  ç›®æ ‡æ ·æœ¬æ•°: {num_sentences}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  å¹¶è¡Œåº¦: {max_workers}")
        print(f"  é¢„è®¡æ—¶é—´: {num_sentences * 3 / max_workers / 60:.1f} åˆ†é’Ÿ")
        print(f"\n{'='*70}\n")

        start_time = time.time()

        for batch_idx in range(num_batches):
            current_batch_size = min(batch_size, num_sentences - len(all_samples))

            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}")
            print(f"{'-'*70}")

            # ç”Ÿæˆå¥å­
            print(f"  ğŸ”„ ç”Ÿæˆ {current_batch_size} ä¸ªå¥å­...")
            sentences = self.generate_sentences(current_batch_size)

            if not sentences:
                print("  âŒ å¥å­ç”Ÿæˆå¤±è´¥")
                continue

            print(f"  âœ… æˆåŠŸç”Ÿæˆ {len(sentences)} ä¸ªå¥å­")

            # å¹¶è¡Œæ ‡æ³¨
            print(f"  ğŸ·ï¸  å¼€å§‹å¹¶è¡Œæ ‡æ³¨ (å¹¶è¡Œåº¦={max_workers})...")
            results = self.annotate_batch_parallel(sentences, max_workers)

            # æ”¶é›†æœ‰æ•ˆæ ·æœ¬
            valid_count = 0
            for i, result in enumerate(results):
                if result:
                    words, tags = result
                    sentence = ''.join(words)

                    # å»é‡
                    if sentence not in seen_sentences:
                        all_samples.append((words, tags))
                        seen_sentences.add(sentence)
                        valid_count += 1

            print(f"  âœ… æœ‰æ•ˆæ ·æœ¬: {valid_count}/{len(results)}")
            print(f"  ğŸ“ˆ ç´¯è®¡æ ·æœ¬: {len(all_samples)}/{num_sentences}")
            print()

            # è¾¾åˆ°ç›®æ ‡æ•°é‡
            if len(all_samples) >= num_sentences:
                break

        elapsed = time.time() - start_time

        # æˆªå–åˆ°ç›®æ ‡æ•°é‡
        all_samples = all_samples[:num_sentences]

        # ä¿å­˜æ•°æ®
        print(f"{'='*70}")
        print(f"ğŸ’¾ ä¿å­˜æ•°æ®...")
        print(f"{'-'*70}")

        with open(output_file, 'w', encoding='utf-8') as f:
            for words, tags in all_samples:
                f.write(self.format_sample(words, tags))
                f.write('\n\n')

        print(f"  âœ… å·²ä¿å­˜ {len(all_samples)} ä¸ªæ ·æœ¬åˆ°: {output_file}")

        # ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(all_samples, elapsed)

        return len(all_samples)

    def print_statistics(self, samples, elapsed):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*70}")
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡")
        print(f"{'='*70}")

        # åŸºç¡€ç»Ÿè®¡
        print(f"\n  â±ï¸  è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        print(f"  âš¡ é€Ÿåº¦: {len(samples)/(elapsed/60):.1f} å¥/åˆ†é’Ÿ")
        print(f"  âœ… æˆåŠŸæ ·æœ¬: {len(samples)}")

        # ä¿®å¤ç»Ÿè®¡
        print(f"\n  ğŸ”§ ä¿®å¤ç»Ÿè®¡:")
        print(f"    æ ¼å¼ä¿®å¤: {self.stats['format_fixed']} æ¬¡")
        print(f"    è¾¹ç•Œä¿®å¤: {self.stats['boundary_fixed']} æ¬¡")
        print(f"    æ— æ•ˆæ ·æœ¬: {self.stats['invalid_samples']} ä¸ª")
        print(f"    APIé”™è¯¯: {self.stats['api_errors']} æ¬¡")

        # æ ‡ç­¾åˆ†å¸ƒ
        tag_counter = Counter()
        entity_counter = Counter()

        for words, tags in samples:
            for tag in tags:
                tag_counter[tag] += 1
                if tag.startswith('B-'):
                    entity_counter[tag[2:]] += 1

        print(f"\n  ğŸ·ï¸  æ ‡ç­¾åˆ†å¸ƒ:")
        total_tags = sum(tag_counter.values())
        for tag in ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'O']:
            count = tag_counter[tag]
            percentage = count / total_tags * 100
            print(f"    {tag:10s}: {count:5d} ({percentage:5.2f}%)")

        print(f"\n  ğŸ“Œ å®ä½“åˆ†å¸ƒ:")
        total_entities = sum(entity_counter.values())
        for entity_type in ['PER', 'LOC', 'ORG']:
            count = entity_counter[entity_type]
            percentage = count / total_entities * 100 if total_entities > 0 else 0
            print(f"    {entity_type:5s}: {count:4d} ({percentage:5.2f}%)")

        # æ ·æœ¬ç¤ºä¾‹
        print(f"\n  ğŸ“– æ ·æœ¬ç¤ºä¾‹ (å‰3ä¸ª):")
        print(f"  {'-'*66}")

        for i, (words, tags) in enumerate(samples[:3], 1):
            sentence = ''.join(words)
            print(f"\n  [{i}] {sentence}")
            print(f"      ", end="")

            j = 0
            while j < len(words):
                if tags[j].startswith('B-'):
                    entity_type = tags[j][2:]
                    entity_chars = [words[j]]
                    k = j + 1
                    while k < len(tags) and tags[k] == f'I-{entity_type}':
                        entity_chars.append(words[k])
                        k += 1
                    entity_text = ''.join(entity_chars)
                    print(f"[{entity_type}:{entity_text}]", end=" ")
                    j = k
                else:
                    print(words[j], end="")
                    j += 1
            print()


def split_train_test(input_file, train_file='data/train.txt',
                     test_file='data/test.txt', test_ratio=0.2):
    """åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†"""
    sentences = []
    current = []

    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                current.append(line)
            elif current:
                sentences.append(current)
                current = []

    if current:
        sentences.append(current)

    random.shuffle(sentences)

    split_idx = int(len(sentences) * (1 - test_ratio))
    train_sentences = sentences[:split_idx]
    test_sentences = sentences[split_idx:]

    with open(train_file, 'w', encoding='utf-8') as f:
        for sent in train_sentences:
            f.write('\n'.join(sent) + '\n\n')

    with open(test_file, 'w', encoding='utf-8') as f:
        for sent in test_sentences:
            f.write('\n'.join(sent) + '\n\n')

    print(f"\n{'='*70}")
    print(f"ğŸ“‚ æ•°æ®é›†åˆ†å‰²")
    print(f"{'-'*70}")
    print(f"  è®­ç»ƒé›†: {len(train_sentences)} ä¸ªæ ·æœ¬ â†’ {train_file}")
    print(f"  æµ‹è¯•é›†: {len(test_sentences)} ä¸ªæ ·æœ¬ â†’ {test_file}")
    print(f"{'='*70}")


def main():
    """ä¸»å‡½æ•° - äº¤äº’å¼ç‰ˆæœ¬"""
    print("=" * 70)
    print(" " * 15 + "DeepSeek NER ä¸¥æ ¼æ•°æ®ç”Ÿæˆå™¨")
    print("=" * 70)

    # è·å–API Key
    api_key = os.getenv('DEEPSEEK_API_KEY')

    if not api_key:
        print("\nğŸ”‘ è¯·è¾“å…¥ DeepSeek API Key:")
        print("   (æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY=sk-xxx)")
        api_key = input("\nAPI Key: ").strip()

    if not api_key:
        print("\nâŒ é”™è¯¯: æœªæä¾›APIå¯†é’¥")
        return

    print(f"\nâœ… API Key: {api_key[:10]}...{api_key[-4:]}")

    # è¯¢é—®ç”Ÿæˆæ•°é‡
    print("\n" + "=" * 70)
    print("âš™ï¸  é…ç½®å‚æ•°")
    print("=" * 70)

    while True:
        num_input = input("\nğŸ“ ç”Ÿæˆå¤šå°‘ä¸ªå¥å­? (æ¨è200-500): ").strip()

        if not num_input:
            num_sentences = 200
            print(f"   ä½¿ç”¨é»˜è®¤å€¼: {num_sentences}")
            break

        try:
            num_sentences = int(num_input)
            if num_sentences < 10:
                print("   âš ï¸  è‡³å°‘éœ€è¦10ä¸ªå¥å­")
                continue
            elif num_sentences > 2000:
                confirm = input(f"   âš ï¸  {num_sentences}ä¸ªå¥å­è¾ƒå¤šï¼Œç¡®è®¤? (y/n): ").strip().lower()
                if confirm == 'y':
                    break
                else:
                    continue
            else:
                break
        except ValueError:
            print("   âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

    # è¯¢é—®å¹¶è¡Œåº¦
    while True:
        workers_input = input("\nâš¡ å¹¶è¡Œåº¦? (æ¨è5-10ï¼Œè¶Šå¤§è¶Šå¿«ä½†APIå‹åŠ›è¶Šå¤§): ").strip()

        if not workers_input:
            max_workers = 5
            print(f"   ä½¿ç”¨é»˜è®¤å€¼: {max_workers}")
            break

        try:
            max_workers = int(workers_input)
            if max_workers < 1:
                print("   âš ï¸  è‡³å°‘éœ€è¦1")
                continue
            elif max_workers > 20:
                print("   âš ï¸  å¹¶è¡Œåº¦è¿‡é«˜å¯èƒ½å¯¼è‡´APIé™æµ")
                confirm = input(f"   ç¡®è®¤ä½¿ç”¨ {max_workers}? (y/n): ").strip().lower()
                if confirm == 'y':
                    break
                else:
                    continue
            else:
                break
        except ValueError:
            print("   âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

    # è¯¢é—®æ‰¹æ¬¡å¤§å°
    batch_size = 50
    advanced = input("\nğŸ”§ éœ€è¦é«˜çº§é…ç½®å—? (y/n, é»˜è®¤n): ").strip().lower()

    if advanced == 'y':
        while True:
            batch_input = input(f"\nğŸ“¦ æ‰¹æ¬¡å¤§å°? (é»˜è®¤50): ").strip()

            if not batch_input:
                batch_size = 50
                break

            try:
                batch_size = int(batch_input)
                if 10 <= batch_size <= 100:
                    break
                else:
                    print("   âš ï¸  å»ºè®®èŒƒå›´: 10-100")
            except ValueError:
                print("   âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

    # ç¡®è®¤é…ç½®
    print("\n" + "=" * 70)
    print("ğŸ“‹ é…ç½®æ‘˜è¦")
    print("=" * 70)
    print(f"  ç”Ÿæˆå¥å­æ•°: {num_sentences}")
    print(f"  å¹¶è¡Œåº¦: {max_workers}")
    print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"  é¢„è®¡æ—¶é—´: {num_sentences * 3 / max_workers / 60:.1f} åˆ†é’Ÿ")
    print(f"  é¢„è®¡APIè°ƒç”¨: ~{num_sentences * 2} æ¬¡")
    print("=" * 70)

    confirm = input("\nâœ… ç¡®è®¤å¼€å§‹ç”Ÿæˆ? (y/n): ").strip().lower()

    if confirm != 'y':
        print("\nâŒ å·²å–æ¶ˆ")
        return

    # åˆ›å»ºç”Ÿæˆå™¨
    generator = StrictDeepSeekDataGenerator(api_key)

    # ç”Ÿæˆæ•°æ®
    print("\n" + "=" * 70)
    print("ğŸš€ å¼€å§‹ç”Ÿæˆ...")
    print("=" * 70)

    try:
        num_generated = generator.generate_dataset(
            num_sentences=num_sentences,
            output_file='data/generated_data.txt',
            batch_size=batch_size,
            max_workers=max_workers
        )

        # åˆ†å‰²æ•°æ®é›†
        if num_generated > 0:
            split = input("\nğŸ“‚ æ˜¯å¦è‡ªåŠ¨åˆ†å‰²è®­ç»ƒ/æµ‹è¯•é›†? (y/n, é»˜è®¤y): ").strip().lower()

            if split != 'n':
                test_ratio_input = input("   æµ‹è¯•é›†æ¯”ä¾‹? (0.1-0.3, é»˜è®¤0.2): ").strip()

                try:
                    test_ratio = float(test_ratio_input) if test_ratio_input else 0.2
                    test_ratio = max(0.1, min(0.3, test_ratio))
                except:
                    test_ratio = 0.2

                split_train_test('data/generated_data.txt', test_ratio=test_ratio)

        print(f"\n{'=' * 70}")
        print(f"âœ… å…¨éƒ¨å®Œæˆï¼")
        print(f"{'=' * 70}")
        print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"  ğŸ“„ åŸå§‹æ•°æ®: data/generated_data.txt")

        if split != 'n':
            print(f"  ğŸ“„ è®­ç»ƒé›†: data/train.txt")
            print(f"  ğŸ“„ æµ‹è¯•é›†: data/test.txt")

        print(f"\nä¸‹ä¸€æ­¥:")
        print(f"  1. æŸ¥çœ‹æ•°æ®:")
        print(f"     head -50 data/train.txt")
        print(f"\n  2. æ£€æŸ¥æ•°æ®è´¨é‡:")
        print(f"     python analyze_data.py")
        print(f"\n  3. è®­ç»ƒæ¨¡å‹:")
        print(f"     python train.py")
        print(f"{'=' * 70}\n")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œå·²åœæ­¢ç”Ÿæˆ")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")


if __name__ == '__main__':
    main()