"""
ç”Ÿæˆé«˜è´¨é‡çš„NERè®­ç»ƒæ•°æ® - ä¿®å¤ç‰ˆ
"""

import random

# å®ä½“åº“
ENTITIES = {
    'PER': [
        'é©¬äº‘', 'é©¬åŒ–è…¾', 'æå½¦å®', 'åˆ˜å¼ºä¸œ', 'é›·å†›', 'å¼ ä¸€é¸£',
        'ä»»æ­£é', 'è‘£æ˜ç ', 'å®—åº†å', 'æŸ³ä¼ å¿—',
        'è¢éš†å¹³', 'é’Ÿå—å±±', 'å± å‘¦å‘¦', 'é’±å­¦æ£®', 'é‚“ç¨¼å…ˆ',
        'é²è¿…', 'è€èˆ', 'è«è¨€', 'åˆ˜æ…ˆæ¬£', 'é‡‘åº¸',
        'å§šæ˜', 'åˆ˜ç¿”', 'æå¨œ', 'å­™æ¨', 'è‹ç‚³æ·»',
        'å‘¨æ°ä¼¦', 'ç‹è²', 'åˆ˜å¾·å', 'å¼ å­¦å‹', 'é‚“ä¸½å›',
        'å¼ è‰ºè°‹', 'é™ˆå‡¯æ­Œ', 'å†¯å°åˆš', 'æå®‰', 'è´¾æ¨ŸæŸ¯',
        'æ¯›æ³½ä¸œ', 'é‚“å°å¹³', 'å‘¨æ©æ¥', 'ä¹ è¿‘å¹³', 'æå…‹å¼º',
        'ç‰›æ¬¢', 'å¼ ä¼Ÿ', 'ç‹èŠ³', 'ææ˜', 'åˆ˜æ´‹', 'é™ˆæ™¨', 'èµµä¸½', 'å­™æ‚¦'
    ],
    'LOC': [
        'åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æˆéƒ½', 'æ­¦æ±‰',
        'è¥¿å®‰', 'é‡åº†', 'å¤©æ´¥', 'è‹å·', 'é•¿æ²™', 'é’å²›', 'å¤§è¿', 'å¦é—¨',
        'é•¿æ±Ÿ', 'é»„æ²³', 'ç æ±Ÿ', 'æ·®æ²³', 'é»‘é¾™æ±Ÿ', 'æ¾èŠ±æ±Ÿ',
        'æ³°å±±', 'é»„å±±', 'å³¨çœ‰å±±', 'åå±±', 'è¡¡å±±', 'æ’å±±', 'åµ©å±±',
        'é•¿åŸ', 'æ•…å®«', 'å¤©å®‰é—¨', 'é¢å’Œå›­', 'åœ†æ˜å›­', 'å¤©å›',
        'è¥¿æ¹–', 'æ¼“æ±Ÿ', 'ä¹å¯¨æ²Ÿ', 'é»„æœæ ‘ç€‘å¸ƒ', 'æ¡‚æ—', 'å¼ å®¶ç•Œ',
        'ä¸­å›½', 'ç¾å›½', 'æ—¥æœ¬', 'è‹±å›½', 'æ³•å›½', 'å¾·å›½', 'ä¿„ç½—æ–¯',
        'ä¸­åŒ—å¤§å­¦', 'æ¸…åå›­', 'æœªåæ¹–', 'å˜‰é™µæ±Ÿ', 'æ´ªå´–æ´', 'å¤ªåŸ'
    ],
    'ORG': [
        'é˜¿é‡Œå·´å·´', 'è…¾è®¯', 'ç™¾åº¦', 'åä¸º', 'å°ç±³', 'å­—èŠ‚è·³åŠ¨',
        'äº¬ä¸œ', 'ç¾å›¢', 'æ‹¼å¤šå¤š', 'æ»´æ»´å‡ºè¡Œ', 'æ¯”äºšè¿ª',
        'ä¸­å›½é“¶è¡Œ', 'å·¥å•†é“¶è¡Œ', 'å»ºè®¾é“¶è¡Œ', 'å†œä¸šé“¶è¡Œ', 'æ‹›å•†é“¶è¡Œ',
        'æ¸…åå¤§å­¦', 'åŒ—äº¬å¤§å­¦', 'å¤æ—¦å¤§å­¦', 'æµ™æ±Ÿå¤§å­¦', 'å—äº¬å¤§å­¦',
        'ä¸­åŒ—å¤§å­¦', 'å±±è¥¿å¤§å­¦', 'å¤ªåŸç†å·¥å¤§å­¦',
        'ä¸­å›½ç§‘å­¦é™¢', 'ä¸­å›½å·¥ç¨‹é™¢', 'ç§‘æŠ€éƒ¨', 'æ•™è‚²éƒ¨',
        'å›½å®¶åšç‰©é¦†', 'æ•…å®«åšç‰©é™¢', 'ä¸­å›½ç¾æœ¯é¦†',
        'ä¸­å¤®ç”µè§†å°', 'äººæ°‘æ—¥æŠ¥', 'æ–°åç¤¾', 'å…‰æ˜æ—¥æŠ¥',
        'ä¸­å›½èˆªå¤©å±€', 'ä¸­å›½èˆªç©ºå…¬å¸', 'ä¸­å›½é“è·¯æ€»å…¬å¸',
        'è”åˆå›½', 'ä¸–ç•Œå«ç”Ÿç»„ç»‡', 'å›½é™…å¥¥å§”ä¼š'
    ]
}

# å¥å­æ¨¡æ¿
TEMPLATES = [
    # å•å®ä½“
    ('{PER}æ˜¯ä¸€ä½æ°å‡ºçš„ç§‘å­¦å®¶', ['PER']),
    ('{PER}æ¥è‡ª{LOC}', ['PER', 'LOC']),
    ('{PER}åœ¨{ORG}å·¥ä½œ', ['PER', 'ORG']),
    ('{PER}åœ¨{ORG}å­¦ä¹ ', ['PER', 'ORG']),
    ('{LOC}æ˜¯ä¸€ä¸ªç¾ä¸½çš„åŸå¸‚', ['LOC']),
    ('{ORG}æ˜¯çŸ¥åä¼ä¸š', ['ORG']),
    ('{ORG}å‘å±•è¿…é€Ÿ', ['ORG']),

    # åŒå®ä½“
    ('{PER}åˆ›ç«‹äº†{ORG}', ['PER', 'ORG']),
    ('{PER}æ¯•ä¸šäº{ORG}', ['PER', 'ORG']),
    ('{PER}åœ¨{LOC}å‡ºç”Ÿ', ['PER', 'LOC']),
    ('{PER}ç”Ÿæ´»åœ¨{LOC}', ['PER', 'LOC']),
    ('{ORG}ä½äº{LOC}', ['ORG', 'LOC']),
    ('{ORG}æ€»éƒ¨åœ¨{LOC}', ['ORG', 'LOC']),
    ('{ORG}åè½åœ¨{LOC}', ['ORG', 'LOC']),
    ('{PER}å°±è¯»äº{ORG}', ['PER', 'ORG']),

    # ä¸‰å®ä½“
    ('{PER}åœ¨{LOC}åˆ›åŠäº†{ORG}', ['PER', 'LOC', 'ORG']),
    ('{PER}ä»{ORG}æ¬åˆ°äº†{LOC}', ['PER', 'ORG', 'LOC']),
    ('{ORG}çš„{PER}æ¥è‡ª{LOC}', ['ORG', 'PER', 'LOC']),
    ('{PER}åœ¨{LOC}çš„{ORG}å·¥ä½œ', ['PER', 'LOC', 'ORG']),

    # å¤æ‚å¥å¼
    ('{PER}å’Œ{PER}ä¸€èµ·åˆ›ç«‹äº†{ORG}', ['PER', 'PER', 'ORG']),
    ('{PER}ä»{LOC}æ¥åˆ°{LOC}å·¥ä½œ', ['PER', 'LOC', 'LOC']),
    ('{ORG}ä¸{ORG}è¾¾æˆåˆä½œ', ['ORG', 'ORG']),
    ('{PER}æ›¾åœ¨{ORG}å’Œ{ORG}ä»»èŒ', ['PER', 'ORG', 'ORG']),

    # çœŸå®åœºæ™¯
    ('{PER}åœ¨{ORG}æ‹…ä»»é¦–å¸­æ‰§è¡Œå®˜', ['PER', 'ORG']),
    ('{PER}æ•™æˆåœ¨{ORG}è¿›è¡Œç ”ç©¶', ['PER', 'ORG']),
    ('{LOC}çš„{ORG}éå¸¸æœ‰å', ['LOC', 'ORG']),
    ('{PER}è®¿é—®äº†{LOC}çš„{ORG}', ['PER', 'LOC', 'ORG']),
    ('{PER}è€ƒä¸Šäº†{ORG}', ['PER', 'ORG']),
    ('{PER}å»{LOC}æ—…æ¸¸', ['PER', 'LOC']),
]


def generate_sentence():
    """ç”Ÿæˆä¸€ä¸ªè®­ç»ƒæ ·æœ¬"""
    template, entity_types = random.choice(TEMPLATES)

    # ä¸ºæ¯ä¸ªå ä½ç¬¦é€‰æ‹©å®ä½“ï¼ˆå¤„ç†é‡å¤ç±»å‹ï¼‰
    entity_counter = {}
    entity_values = []

    for etype in entity_types:
        # ç»Ÿè®¡æ¯ç§ç±»å‹å‡ºç°çš„æ¬¡æ•°
        if etype not in entity_counter:
            entity_counter[etype] = 0
        entity_counter[etype] += 1

        # é€‰æ‹©å®ä½“
        entity = random.choice(ENTITIES[etype])
        entity_values.append((etype, entity))

    # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    sentence = template
    entity_positions = []  # å­˜å‚¨ (èµ·å§‹ä½ç½®, å®ä½“æ–‡æœ¬, å®ä½“ç±»å‹)

    for etype, entity in entity_values:
        placeholder = f'{{{etype}}}'
        if placeholder in sentence:
            pos = sentence.find(placeholder)
            entity_positions.append((pos, entity, etype))
            sentence = sentence.replace(placeholder, entity, 1)

    # ç”ŸæˆBIOæ ‡æ³¨
    words = list(sentence)
    tags = ['O'] * len(words)

    # æŒ‰ä½ç½®æ’åºï¼ˆå› ä¸ºæ›¿æ¢åä½ç½®ä¼šå˜åŒ–ï¼‰
    # éœ€è¦é‡æ–°è®¡ç®—å®é™…ä½ç½®
    current_sentence = template
    current_pos = 0

    for etype, entity in entity_values:
        placeholder = f'{{{etype}}}'
        if placeholder in current_sentence:
            # æ‰¾åˆ°å ä½ç¬¦ä½ç½®
            placeholder_pos = current_sentence.find(placeholder)

            # è®¡ç®—å®ä½“åœ¨æœ€ç»ˆå¥å­ä¸­çš„å®é™…ä½ç½®
            actual_pos = placeholder_pos + (current_pos - placeholder_pos)

            # æ ‡æ³¨ B-TYPE å’Œ I-TYPE
            for i, char in enumerate(entity):
                if i == 0:
                    tags[actual_pos + i] = f'B-{etype}'
                else:
                    tags[actual_pos + i] = f'I-{etype}'

            # æ›´æ–°å¥å­å’Œä½ç½®
            current_sentence = current_sentence.replace(placeholder, entity, 1)
            current_pos = actual_pos + len(entity)

    return words, tags


def generate_dataset(num_samples=2000, train_ratio=0.8):
    """ç”Ÿæˆæ•°æ®é›†"""
    print(f"ğŸ”„ ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬...")

    samples = []
    seen_sentences = set()  # å»é‡

    attempts = 0
    max_attempts = num_samples * 3  # æœ€å¤šå°è¯•3å€æ¬¡æ•°

    while len(samples) < num_samples and attempts < max_attempts:
        attempts += 1
        try:
            words, tags = generate_sentence()
            sentence_str = ''.join(words)

            # å»é‡
            if sentence_str not in seen_sentences:
                samples.append((words, tags))
                seen_sentences.add(sentence_str)

                # è¿›åº¦æç¤º
                if len(samples) % 500 == 0:
                    print(f"  å·²ç”Ÿæˆ {len(samples)}/{num_samples} ä¸ªæ ·æœ¬...")
        except Exception as e:
            print(f"  âš ï¸  ç”Ÿæˆå¤±è´¥: {e}")
            continue

    print(f"  âœ… æˆåŠŸç”Ÿæˆ {len(samples)} ä¸ªæ ·æœ¬")

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    random.shuffle(samples)
    split_idx = int(len(samples) * train_ratio)
    train_samples = samples[:split_idx]
    test_samples = samples[split_idx:]

    return train_samples, test_samples


def save_dataset(samples, filename):
    """ä¿å­˜æ•°æ®é›†"""
    with open(filename, 'w', encoding='utf-8') as f:
        for words, tags in samples:
            for word, tag in zip(words, tags):
                f.write(f'{word} {tag}\n')
            f.write('\n')

    print(f"  âœ… ä¿å­˜ {len(samples)} ä¸ªæ ·æœ¬åˆ° {filename}")


def show_samples(samples, num=5):
    """æ˜¾ç¤ºæ ·æœ¬ç¤ºä¾‹"""
    print(f"\nğŸ“– æ ·æœ¬ç¤ºä¾‹ (å‰{num}ä¸ª):")
    print("-"*70)

    for i, (words, tags) in enumerate(samples[:num], 1):
        sentence = ''.join(words)
        print(f"\n[{i}] {sentence}")
        print(f"    ", end="")

        j = 0
        while j < len(words):
            if tags[j].startswith('B-'):
                entity_type = tags[j][2:]
                entity_chars = [words[j]]
                k = j + 1
                while k < len(tags) and tags[k] == f'I-{entity_type}':
                    entity_chars.append(words[k])
                    k += 1
                entity_text = ''.join(entity_chars)
                print(f"[{entity_type}:{entity_text}]", end=" ")
                j = k
            else:
                print(words[j], end="")
                j += 1
        print()


def main():
    print("="*70)
    print(" "*20 + "ç”Ÿæˆé«˜è´¨é‡NERæ•°æ®")
    print("="*70)

    # ç”Ÿæˆæ•°æ®
    train_samples, test_samples = generate_dataset(num_samples=2000)

    # æ˜¾ç¤ºæ ·æœ¬
    show_samples(train_samples, num=10)

    # ä¿å­˜
    print("\n" + "="*70)
    print("ğŸ’¾ ä¿å­˜æ•°æ®...")
    print("="*70)
    save_dataset(train_samples, 'data/train_new.txt')
    save_dataset(test_samples, 'data/test_new.txt')

    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*70)
    print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
    print("="*70)
    print(f"  è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_samples)} ä¸ªæ ·æœ¬")

    # å®ä½“ç»Ÿè®¡
    from collections import Counter
    entity_counter = Counter()

    for _, tags in train_samples:
        for tag in tags:
            if tag != 'O':
                entity_type = tag.split('-')[1]
                entity_counter[entity_type] += 1

    print(f"\n  å®ä½“åˆ†å¸ƒ (è®­ç»ƒé›†):")
    total_entities = sum(entity_counter.values())
    for entity_type, count in sorted(entity_counter.items()):
        percentage = count / total_entities * 100
        print(f"    {entity_type}: {count} ({percentage:.1f}%)")

    print("\n" + "="*70)
    print("âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print("="*70)
    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®:")
    print("     cat data/train_new.txt | head -50")
    print("\n  2. ä½¿ç”¨æ–°æ•°æ®:")
    print("     mv data/train.txt data/train_old.txt")
    print("     mv data/test.txt data/test_old.txt")
    print("     mv data/train_new.txt data/train.txt")
    print("     mv data/test_new.txt data/test.txt")
    print("\n  3. é‡æ–°è®­ç»ƒ:")
    print("     python train.py")
    print("="*70)


if __name__ == '__main__':
    main()