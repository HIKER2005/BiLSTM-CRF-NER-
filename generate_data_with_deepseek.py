# """
# ä½¿ç”¨DeepSeek APIç”ŸæˆNERè®­ç»ƒæ•°æ®
# """
#
# import os
# import requests
# import json
# import time
# from tqdm import tqdm
# import re
#
#
# class DeepSeekDataGenerator:
#     def __init__(self, api_key, base_url="https://api.deepseek.com/v1"):
#         """
#         åˆå§‹åŒ–DeepSeekæ•°æ®ç”Ÿæˆå™¨
#
#         Args:
#             api_key: DeepSeek APIå¯†é’¥
#             base_url: APIåŸºç¡€URL
#         """
#         self.api_key = api_key
#         self.base_url = base_url
#         self.headers = {
#             "Authorization": f"Bearer {api_key}",
#             "Content-Type": "application/json"
#         }
#
#     def call_api(self, messages, temperature=0.7, max_tokens=2000):
#         """è°ƒç”¨DeepSeek API"""
#         url = f"{self.base_url}/chat/completions"
#
#         payload = {
#             "model": "deepseek-chat",
#             "messages": messages,
#             "temperature": temperature,
#             "max_tokens": max_tokens
#         }
#
#         try:
#             response = requests.post(url, headers=self.headers, json=payload, timeout=30)
#             response.raise_for_status()
#             result = response.json()
#             return result['choices'][0]['message']['content']
#         except Exception as e:
#             print(f"APIè°ƒç”¨é”™è¯¯: {e}")
#             return None
#
#     def generate_sentences(self, num_sentences=50, domains=None):
#         """ç”ŸæˆåŒ…å«å®ä½“çš„å¥å­"""
#         if domains is None:
#             domains = ['æ–°é—»', 'ç§‘æŠ€', 'å¨±ä¹', 'ä½“è‚²', 'å•†ä¸š', 'å†å²', 'æ–‡åŒ–']
#
#         domain_str = 'ã€'.join(domains)
#
#         prompt = f"""è¯·ç”Ÿæˆ{num_sentences}ä¸ªä¸­æ–‡å¥å­ï¼Œè¦æ±‚ï¼š
#
# 1. å¥å­é•¿åº¦åœ¨10-30ä¸ªå­—ä¹‹é—´
# 2. æ¯ä¸ªå¥å­å¿…é¡»åŒ…å«è‡³å°‘ä¸€ä¸ªå‘½åå®ä½“ï¼ˆäººåã€åœ°åæˆ–æœºæ„åï¼‰
# 3. æ¶µç›–ä»¥ä¸‹é¢†åŸŸï¼š{domain_str}
# 4. å¥å­è¦è‡ªç„¶ã€çœŸå®ã€å¤šæ ·åŒ–
# 5. å®ä½“ç±»å‹è¦å‡è¡¡åˆ†å¸ƒ
#
# æ¯è¡Œä¸€ä¸ªå¥å­ï¼Œä¸è¦ç¼–å·ï¼Œç›´æ¥è¾“å‡ºå¥å­ã€‚
#
# ç¤ºä¾‹ï¼š
# é©¬äº‘åˆ›ç«‹äº†é˜¿é‡Œå·´å·´é›†å›¢
# å§šæ˜æ˜¯ä¸­å›½è‘—åçš„ç¯®çƒè¿åŠ¨å‘˜
# æ•…å®«ä½äºåŒ—äº¬å¸‚ä¸­å¿ƒ
# è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°æ¬¾iPhone
#
# ç°åœ¨è¯·ç”Ÿæˆ{num_sentences}ä¸ªå¥å­ï¼š"""
#
#
#         messages = [{"role": "user", "content": prompt}]
#
#         response = self.call_api(messages, temperature=0.8)
#
#         if response:
#             sentences = []
#             for line in response.strip().split('\n'):
#                 line = line.strip()
#                 line = re.sub(r'^\d+[\.\ã€\s]+', '', line)
#                 if line and len(line) >= 5:
#                     sentences.append(line)
#             return sentences
#
#         return []
#
#     def annotate_sentence(self, sentence):
#         """å¯¹å•ä¸ªå¥å­è¿›è¡Œå®ä½“æ ‡æ³¨"""
#         prompt = f"""è¯·å¯¹ä¸‹é¢çš„ä¸­æ–‡å¥å­è¿›è¡Œå‘½åå®ä½“è¯†åˆ«æ ‡æ³¨ï¼Œä½¿ç”¨BIOæ ‡æ³¨æ ¼å¼ã€‚
#
# å®ä½“ç±»å‹ï¼š
# - PER: äººåï¼ˆå¦‚ï¼šé©¬äº‘ã€å§šæ˜ã€æç™½ï¼‰
# - LOC: åœ°åï¼ˆå¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·ã€æ­å·ã€ä¸­å›½ï¼‰
# - ORG: æœºæ„åï¼ˆå¦‚ï¼šé˜¿é‡Œå·´å·´ã€æ¸…åå¤§å­¦ã€è”åˆå›½ï¼‰
#
# æ ‡æ³¨æ ¼å¼ï¼š
# - B-XXX: å®ä½“å¼€å§‹
# - I-XXX: å®ä½“å†…éƒ¨
# - O: éå®ä½“
#
# è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
# 1. æ¯è¡Œä¸€ä¸ªå­—å’Œå¯¹åº”çš„æ ‡ç­¾ï¼Œç”¨ç©ºæ ¼åˆ†éš”
# 2. æŒ‰ç…§å¥å­é¡ºåºé€å­—æ ‡æ³¨
# 3. ä¸è¦æœ‰ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–è¯´æ˜
# 4. æ ‡æ³¨è¦å‡†ç¡®ã€å®Œæ•´
#
# ç¤ºä¾‹è¾“å…¥ï¼šé©¬äº‘åˆ›ç«‹äº†é˜¿é‡Œå·´å·´
# ç¤ºä¾‹è¾“å‡ºï¼š
# é©¬ B-PER
# äº‘ I-PER
# åˆ› O
# ç«‹ O
# äº† O
# é˜¿ B-ORG
# é‡Œ I-ORG
# å·´ I-ORG
# å·´ I-ORG
#
# ç°åœ¨è¯·æ ‡æ³¨ä»¥ä¸‹å¥å­ï¼š{sentence}
#
# è¾“å‡ºï¼š"""
#
#         messages = [{"role": "user", "content": prompt}]
#
#         response = self.call_api(messages, temperature=0.3)
#
#         if response:
#             lines = []
#             for line in response.strip().split('\n'):
#                 line = line.strip()
#                 if ' ' in line:
#                     parts = line.split()
#                     if len(parts) == 2 and len(parts[0]) == 1:
#                         lines.append(line)
#
#             annotated_chars = ''.join([line.split()[0] for line in lines])
#             if annotated_chars == sentence.replace(' ', ''):
#                 return '\n'.join(lines)
#             else:
#                 print(f"è­¦å‘Šï¼šæ ‡æ³¨ä¸å®Œæ•´ - {sentence}")
#                 return None
#
#         return None
#
#     def generate_dataset(self, num_sentences=100, output_file='data/generated_data.txt',
#                          batch_size=10, delay=1.0):
#         """ç”Ÿæˆå®Œæ•´çš„æ•°æ®é›†"""
#         os.makedirs(os.path.dirname(output_file), exist_ok=True)
#
#         all_data = []
#         num_batches = (num_sentences + batch_size - 1) // batch_size
#
#         print(f"å¼€å§‹ç”Ÿæˆæ•°æ®é›†ï¼Œæ€»å…±{num_sentences}ä¸ªå¥å­ï¼Œåˆ†{num_batches}æ‰¹...")
#
#         for batch_idx in range(num_batches):
#             current_batch_size = min(batch_size, num_sentences - batch_idx * batch_size)
#
#             print(f"\næ‰¹æ¬¡ {batch_idx + 1}/{num_batches}:")
#
#             # ç”Ÿæˆå¥å­
#             print(f"  ç”Ÿæˆ{current_batch_size}ä¸ªå¥å­...")
#             sentences = self.generate_sentences(current_batch_size)
#
#             if not sentences:
#                 print("  å¥å­ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
#                 continue
#
#             print(f"  æˆåŠŸç”Ÿæˆ{len(sentences)}ä¸ªå¥å­")
#
#             # æ ‡æ³¨å¥å­
#             print(f"  å¼€å§‹æ ‡æ³¨...")
#             for idx, sentence in enumerate(tqdm(sentences, desc="  æ ‡æ³¨è¿›åº¦")):
#                 annotated = self.annotate_sentence(sentence)
#
#                 if annotated:
#                     all_data.append(annotated)
#                 else:
#                     print(f"    å¥å­æ ‡æ³¨å¤±è´¥: {sentence}")
#
#                 if idx < len(sentences) - 1:
#                     time.sleep(delay)
#
#             if batch_idx < num_batches - 1:
#                 time.sleep(delay)
#
#         print(f"\næˆåŠŸç”Ÿæˆ{len(all_data)}ä¸ªæ ‡æ³¨æ ·æœ¬")
#
#         with open(output_file, 'w', encoding='utf-8') as f:
#             f.write('\n\n'.join(all_data))
#
#         print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
#
#         return len(all_data)
#
#     def validate_data(self, file_path):
#         """éªŒè¯ç”Ÿæˆçš„æ•°æ®è´¨é‡"""
#         print(f"\néªŒè¯æ•°æ®: {file_path}")
#
#         sentences = []
#         current_sentence = []
#
#         with open(file_path, 'r', encoding='utf-8') as f:
#             for line in f:
#                 line = line.strip()
#                 if line:
#                     current_sentence.append(line)
#                 else:
#                     if current_sentence:
#                         sentences.append(current_sentence)
#                         current_sentence = []
#
#         if current_sentence:
#             sentences.append(current_sentence)
#
#         print(f"  æ€»å¥å­æ•°: {len(sentences)}")
#
#         entity_counts = {'PER': 0, 'LOC': 0, 'ORG': 0}
#         total_tokens = 0
#         entity_tokens = 0
#
#         for sentence in sentences:
#             for line in sentence:
#                 parts = line.split()
#                 if len(parts) == 2:
#                     word, tag = parts
#                     total_tokens += 1
#
#                     if tag != 'O':
#                         entity_tokens += 1
#                         entity_type = tag.split('-')[-1]
#                         if entity_type in entity_counts:
#                             entity_counts[entity_type] += 1
#
#         print(f"  æ€»å­—ç¬¦æ•°: {total_tokens}")
#         print(f"  å®ä½“å­—ç¬¦æ•°: {entity_tokens} ({entity_tokens / total_tokens * 100:.2f}%)")
#         print(f"  å®ä½“ç»Ÿè®¡:")
#         for entity_type, count in entity_counts.items():
#             print(f"    {entity_type}: {count}")
#
#         return {
#             'num_sentences': len(sentences),
#             'total_tokens': total_tokens,
#             'entity_tokens': entity_tokens,
#             'entity_counts': entity_counts
#         }
#
#
# def split_train_test(input_file, train_file='data/train.txt',
#                      test_file='data/test.txt', test_ratio=0.2):
#     """å°†ç”Ÿæˆçš„æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
#     sentences = []
#     current_sentence = []
#
#     with open(input_file, 'r', encoding='utf-8') as f:
#         for line in f:
#             line = line.strip()
#             if line:
#                 current_sentence.append(line)
#             else:
#                 if current_sentence:
#                     sentences.append(current_sentence)
#                     current_sentence = []
#
#     if current_sentence:
#         sentences.append(current_sentence)
#
#     import random
#     random.shuffle(sentences)
#
#     split_idx = int(len(sentences) * (1 - test_ratio))
#     train_sentences = sentences[:split_idx]
#     test_sentences = sentences[split_idx:]
#
#     with open(train_file, 'w', encoding='utf-8') as f:
#         for sentence in train_sentences:
#             f.write('\n'.join(sentence) + '\n\n')
#
#     with open(test_file, 'w', encoding='utf-8') as f:
#         for sentence in test_sentences:
#             f.write('\n'.join(sentence) + '\n\n')
#
#     print(f"\næ•°æ®é›†åˆ†å‰²å®Œæˆ:")
#     print(f"  è®­ç»ƒé›†: {len(train_sentences)} å¥ -> {train_file}")
#     print(f"  æµ‹è¯•é›†: {len(test_sentences)} å¥ -> {test_file}")
#
#
# def main():
#     """ä¸»å‡½æ•°"""
#     print("=" * 60)
#     print(" " * 15 + "DeepSeek NERæ•°æ®ç”Ÿæˆå™¨")
#     print("=" * 60)
#
#     api_key = os.getenv('DEEPSEEK_API_KEY')
#
#     if not api_key:
#         print("\nè¯·è¾“å…¥æ‚¨çš„DeepSeek APIå¯†é’¥:")
#         api_key = input("API Key: ").strip()
#
#         if not api_key:
#             print("é”™è¯¯ï¼šæœªæä¾›APIå¯†é’¥")
#             return
#
#     generator = DeepSeekDataGenerator(api_key)
#
#     num_sentences = int(input("\nè¦ç”Ÿæˆå¤šå°‘ä¸ªå¥å­ï¼Ÿ(æ¨è200-500): ") or "300")
#     batch_size = 20
#     delay = 1.5
#
#     print(f"\né…ç½®:")
#     print(f"  æ€»å¥å­æ•°: {num_sentences}")
#     print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
#     print(f"  è°ƒç”¨é—´éš”: {delay}ç§’")
#
#     output_file = 'data/generated_data.txt'
#     num_generated = generator.generate_dataset(
#         num_sentences=num_sentences,
#         output_file=output_file,
#         batch_size=batch_size,
#         delay=delay
#     )
#
#     if num_generated > 0:
#         stats = generator.validate_data(output_file)
#         split_train_test(output_file, test_ratio=0.2)
#
#         print("\n" + "=" * 60)
#         print("æ•°æ®ç”Ÿæˆå®Œæˆï¼")
#         print("=" * 60)
#         print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œ python train.py å¼€å§‹è®­ç»ƒæ¨¡å‹")
#     else:
#         print("\næ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥å’Œç½‘ç»œè¿æ¥")
#
#
# if __name__ == '__main__':
#     main()
"""
ä½¿ç”¨ DeepSeek API ç”Ÿæˆ NER è®­ç»ƒæ•°æ®
å¸¦ä¸¥æ ¼æ ¼å¼éªŒè¯å’Œè‡ªåŠ¨ä¿®å¤
"""

import os
import json
import re
from openai import OpenAI
from collections import Counter

# DeepSeek API é…ç½®
DEEPSEEK_API_KEY = "your_api_key_here"  # æ›¿æ¢ä¸ºä½ çš„ API Key
DEEPSEEK_BASE_URL = "https://api.deepseek.com"

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(
    api_key=DEEPSEEK_API_KEY,
    base_url=DEEPSEEK_BASE_URL
)


def generate_prompt(num_samples=50):
    """ç”Ÿæˆä¸¥æ ¼çš„ prompt"""
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«(NER)æ•°æ®æ ‡æ³¨ä¸“å®¶ã€‚

    ## ä»»åŠ¡
    ç”Ÿæˆ {num_samples} ä¸ªé«˜è´¨é‡çš„ä¸­æ–‡ NER è®­ç»ƒæ ·æœ¬ã€‚
    
    ## ä¸¥æ ¼è¦æ±‚
    
    ### 1. æ ‡ç­¾ä½“ç³»ï¼ˆåªèƒ½ä½¿ç”¨ä»¥ä¸‹7ç§æ ‡ç­¾ï¼‰
    - **B-PER**: äººåçš„ç¬¬ä¸€ä¸ªå­—ï¼ˆå¦‚ï¼šé©¬äº‘ â†’ é©¬:B-PERï¼‰
    - **I-PER**: äººåçš„åç»­å­—ï¼ˆå¦‚ï¼šé©¬äº‘ â†’ äº‘:I-PERï¼‰
    - **B-LOC**: åœ°åçš„ç¬¬ä¸€ä¸ªå­—
    - **I-LOC**: åœ°åçš„åç»­å­—
    - **B-ORG**: æœºæ„åçš„ç¬¬ä¸€ä¸ªå­—
    - **I-ORG**: æœºæ„åçš„åç»­å­—
    - **O**: éå®ä½“å­—ç¬¦
    
    ### 2. ç»å¯¹ç¦æ­¢çš„æ ¼å¼ï¼ˆä¼šå¯¼è‡´æ•°æ®ä½œåºŸï¼‰
    âŒ å€’åºæ ¼å¼ï¼šPER-B, LOC-I, ORG-B
    âŒ BMESæ ¼å¼ï¼šM-PER, E-ORG, S-LOC
    âŒ æ— å‰ç¼€ï¼šPER, LOC, ORG
    âŒ å…¶ä»–å˜ä½“ï¼šPerson, Location, Organization
    
    ### 3. æ ‡æ³¨è§„åˆ™
    - æ¯ä¸ªå®ä½“çš„ç¬¬ä¸€ä¸ªå­—å¿…é¡»æ˜¯ B- æ ‡ç­¾
    - æ¯ä¸ªå®ä½“çš„åç»­å­—å¿…é¡»æ˜¯ I- æ ‡ç­¾
    - ç›¸åŒç±»å‹å®ä½“å¿…é¡»è¿ç»­æ ‡æ³¨
    - éå®ä½“å­—ç¬¦å¿…é¡»æ ‡æ³¨ä¸º O
    
    ### 4. è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå¾ªï¼‰
    å­— æ ‡ç­¾
    å­— æ ‡ç­¾
    ...
    <ç©ºè¡Œ>
    å­— æ ‡ç­¾
    ...

    ### 5. å®ä½“ç±»å‹ç¤ºä¾‹
    - **äººå(PER)**: é©¬äº‘ã€é©¬åŒ–è…¾ã€æå½¦å®ã€è¢éš†å¹³ã€é²è¿…ã€å§šæ˜ã€å‘¨æ°ä¼¦
    - **åœ°å(LOC)**: åŒ—äº¬ã€ä¸Šæµ·ã€é•¿æ±Ÿã€é»„æ²³ã€æ³°å±±ã€è¥¿æ¹–ã€ä¸­å›½ã€ç¾å›½
    - **æœºæ„(ORG)**: é˜¿é‡Œå·´å·´ã€æ¸…åå¤§å­¦ã€ä¸­å›½é“¶è¡Œã€è”åˆå›½ã€ä¸­å¤®ç”µè§†å°

    ## æ ‡å‡†ç¤ºä¾‹

    ç¤ºä¾‹1:
    é©¬ B-PER
    äº‘ I-PER
    åˆ› O
    ç«‹ O
    äº† O
    é˜¿ B-ORG
    é‡Œ I-ORG
    å·´ I-ORG
    å·´ I-ORG

    ç¤ºä¾‹2:
    è¢ B-PER
    éš† I-PER
    å¹³ I-PER
    åœ¨ O
    æ¹– B-LOC
    å— I-LOC
    å·¥ O
    ä½œ O

    ç¤ºä¾‹3:
    æ¸… B-ORG
    å I-ORG
    å¤§ I-ORG
    å­¦ I-ORG
    ä½ O
    äº O
    åŒ— B-LOC
    äº¬ I-LOC

    ## è¦æ±‚
    1. æ¯ä¸ªå¥å­é•¿åº¦ 8-30 å­—
    2. æ¯ä¸ªå¥å­è‡³å°‘åŒ…å« 1 ä¸ªå®ä½“
    3. å®ä½“ç±»å‹è¦å‡è¡¡åˆ†å¸ƒ
    4. å¥å­è¦ç¬¦åˆä¸­æ–‡è¯­æ³•ï¼Œå†…å®¹çœŸå®åˆç†
    5. ä¸¥æ ¼ä½¿ç”¨ä¸Šè¿°7ç§æ ‡ç­¾ï¼Œä¸å¾—æœ‰ä»»ä½•åå·®

    ç°åœ¨è¯·ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬ï¼Œä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºã€‚
    """
    return prompt


def call_deepseek_api(prompt, model="deepseek-chat", max_retries=3):
    """è°ƒç”¨ DeepSeek API"""
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼éµå¾ªæ ¼å¼è¦æ±‚çš„NERæ•°æ®æ ‡æ³¨ä¸“å®¶ã€‚ä½ åªèƒ½ä½¿ç”¨ B-PER, I-PER, B-LOC, I-LOC, B-ORG, I-ORG, O è¿™7ç§æ ‡ç­¾ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7,  # é€‚åº¦éšæœºæ€§
                max_tokens=4000,
                stream=False
            )

            return response.choices[0].message.content

        except Exception as e:
            print(f"  âš ï¸  API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                raise
            continue


def fix_tag_format(tag):
    """ä¿®å¤æ ‡ç­¾æ ¼å¼"""
    tag = tag.strip().upper()

    # å·²ç»æ˜¯æ­£ç¡®æ ¼å¼
    if tag in ['B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'O']:
        return tag

    # å¤„ç†å€’åºæ ¼å¼: PER-B â†’ B-PER
    if re.match(r'^(PER|LOC|ORG)-([BI])$', tag):
        entity_type, prefix = tag.split('-')
        return f'{prefix}-{entity_type}'

    # å¤„ç†æ— å‰ç¼€: PER â†’ B-PERï¼ˆé»˜è®¤ä¸ºå¼€å§‹ï¼‰
    if tag in ['PER', 'LOC', 'ORG']:
        return f'B-{tag}'

    # å¤„ç† BMES æ ¼å¼
    if re.match(r'^[BMES]-(PER|LOC|ORG)$', tag):
        prefix, entity_type = tag.split('-')
        if prefix == 'B':
            return f'B-{entity_type}'
        elif prefix in ['M', 'E']:  # Må’ŒEéƒ½å½“ä½œI
            return f'I-{entity_type}'
        elif prefix == 'S':  # S(å•å­—å®ä½“)å½“ä½œB
            return f'B-{entity_type}'

    # å¤„ç†å…¨ç§°: PERSON â†’ PER
    tag_mapping = {
        'PERSON': 'PER', 'PEOPLE': 'PER', 'äººå': 'PER',
        'LOCATION': 'LOC', 'PLACE': 'LOC', 'åœ°å': 'LOC',
        'ORGANIZATION': 'ORG', 'COMPANY': 'ORG', 'æœºæ„': 'ORG'
    }

    for old, new in tag_mapping.items():
        if old in tag:
            if 'B' in tag or tag == old:
                return f'B-{new}'
            else:
                return f'I-{new}'

    # æ— æ³•è¯†åˆ«ï¼Œè¿”å› O
    return 'O'


def fix_entity_boundaries(words, tags):
    """ä¿®å¤å®ä½“è¾¹ç•Œé—®é¢˜"""
    fixed_tags = []
    i = 0

    while i < len(tags):
        tag = tags[i]

        # å¤„ç† I- æ ‡ç­¾å‡ºç°åœ¨å¼€å¤´æˆ–è·Ÿåœ¨ O åé¢çš„æƒ…å†µ
        if tag.startswith('I-'):
            entity_type = tag[2:]
            # æ£€æŸ¥å‰ä¸€ä¸ªæ ‡ç­¾
            if i == 0 or not fixed_tags[-1].endswith(f'-{entity_type}'):
                # ä¿®æ­£ä¸º B-
                fixed_tags.append(f'B-{entity_type}')
            else:
                fixed_tags.append(tag)

        # å¤„ç† B- æ ‡ç­¾
        elif tag.startswith('B-'):
            fixed_tags.append(tag)

        # å¤„ç† O æ ‡ç­¾
        else:
            fixed_tags.append('O')

        i += 1

    return fixed_tags


def validate_sample(words, tags):
    """éªŒè¯æ ·æœ¬æœ‰æ•ˆæ€§"""
    if len(words) != len(tags):
        return False, "å­—ç¬¦æ•°å’Œæ ‡ç­¾æ•°ä¸åŒ¹é…"

    if len(words) < 5:
        return False, "å¥å­å¤ªçŸ­"

    if len(words) > 100:
        return False, "å¥å­å¤ªé•¿"

    # æ£€æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªå®ä½“
    has_entity = any(tag.startswith('B-') for tag in tags)
    if not has_entity:
        return False, "æ²¡æœ‰å®ä½“"

    # æ£€æŸ¥æ ‡ç­¾åˆæ³•æ€§
    valid_tags = {'B-PER', 'I-PER', 'B-LOC', 'I-LOC', 'B-ORG', 'I-ORG', 'O'}
    for tag in tags:
        if tag not in valid_tags:
            return False, f"éæ³•æ ‡ç­¾: {tag}"

    return True, "OK"


def parse_api_response(response_text):
    """è§£æ API å“åº”"""
    lines = response_text.strip().split('\n')

    samples = []
    current_words = []
    current_tags = []

    for line in lines:
        line = line.strip()

        # è·³è¿‡ç©ºè¡Œå’Œmarkdownä»£ç å—æ ‡è®°
        if not line or line.startswith('```'):
            if current_words:
                samples.append((current_words, current_tags))
                current_words = []
                current_tags = []
            continue

            # è·³è¿‡è¯´æ˜æ€§æ–‡å­—
        if 'ç¤ºä¾‹' in line or 'è¦æ±‚' in line or line.startswith('#'):
            continue

            # è§£æ "å­— æ ‡ç­¾" æ ¼å¼
        parts = line.split()
        if len(parts) == 2:
            word, tag = parts

            # åªä¿ç•™å•ä¸ªå­—ç¬¦
            if len(word) == 1:
                current_words.append(word)
                current_tags.append(tag)
        elif len(parts) == 1 and len(parts[0]) == 1:
            # å¯èƒ½åªæœ‰å­—ç¬¦ï¼Œæ²¡æœ‰æ ‡ç­¾
            current_words.append(parts[0])
            current_tags.append('O')

            # æ·»åŠ æœ€åä¸€ä¸ªæ ·æœ¬
    if current_words:
        samples.append((current_words, current_tags))

    return samples


def clean_sample(words, tags):
    """æ¸…æ´—å•ä¸ªæ ·æœ¬"""
    # 1. ä¿®å¤æ ‡ç­¾æ ¼å¼
    fixed_tags = [fix_tag_format(tag) for tag in tags]

    # 2. ä¿®å¤å®ä½“è¾¹ç•Œ
    fixed_tags = fix_entity_boundaries(words, fixed_tags)

    return words, fixed_tags


def generate_data_with_deepseek(
        num_samples=200,
        batch_size=50,
        output_train='data/train_deepseek.txt',
        output_test='data/test_deepseek.txt',
        train_ratio=0.8
):
    """ä½¿ç”¨ DeepSeek ç”Ÿæˆæ•°æ®"""

    print("=" * 70)
    print(" " * 15 + "DeepSeek NER æ•°æ®ç”Ÿæˆå™¨")
    print("=" * 70)

    if DEEPSEEK_API_KEY == "your_api_key_here":
        print("\nâŒ é”™è¯¯: è¯·å…ˆé…ç½® DeepSeek API Key")
        print("è¯·åœ¨è„šæœ¬å¼€å¤´è®¾ç½®: DEEPSEEK_API_KEY = 'sk-xxx'")
        return

    all_samples = []
    error_count = 0
    fixed_count = 0

    # åˆ†æ‰¹ç”Ÿæˆ
    num_batches = (num_samples + batch_size - 1) // batch_size

    for batch_idx in range(num_batches):
        current_batch_size = min(batch_size, num_samples - len(all_samples))

        print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{num_batches} (ç›®æ ‡: {current_batch_size} ä¸ªæ ·æœ¬)")
        print("-" * 70)

        # ç”Ÿæˆ prompt
        prompt = generate_prompt(current_batch_size)

        # è°ƒç”¨ API
        print("  ğŸ”„ è°ƒç”¨ DeepSeek API...")
        try:
            response = call_deepseek_api(prompt)
            print("  âœ… API è°ƒç”¨æˆåŠŸ")
        except Exception as e:
            print(f"  âŒ API è°ƒç”¨å¤±è´¥: {e}")
            error_count += 1
            continue

            # è§£æå“åº”
        print("  ğŸ” è§£æå“åº”...")
        samples = parse_api_response(response)
        print(f"  ğŸ“Š è§£æå¾—åˆ° {len(samples)} ä¸ªåŸå§‹æ ·æœ¬")

        # æ¸…æ´—å’ŒéªŒè¯
        print("  ğŸ§¹ æ¸…æ´—å’ŒéªŒè¯æ ·æœ¬...")
        valid_samples = []

        for words, tags in samples:
            # æ¸…æ´—
            words, tags = clean_sample(words, tags)

            # éªŒè¯
            is_valid, msg = validate_sample(words, tags)

            if is_valid:
                valid_samples.append((words, tags))
            else:
                print(f"    âš ï¸  æ ·æœ¬æ— æ•ˆ: {msg} - {''.join(words[:10])}...")
                error_count += 1

        print(f"  âœ… æœ‰æ•ˆæ ·æœ¬: {len(valid_samples)} ä¸ª")

        all_samples.extend(valid_samples)

        print(f"  ğŸ“ˆ ç´¯è®¡æœ‰æ•ˆæ ·æœ¬: {len(all_samples)}/{num_samples}")

        # å¦‚æœå·²ç»è¶³å¤Ÿäº†å°±åœæ­¢
        if len(all_samples) >= num_samples:
            break

            # å»é‡
    print("\nğŸ”„ å»é‡...")
    unique_samples = []
    seen_sentences = set()

    for words, tags in all_samples:
        sentence = ''.join(words)
        if sentence not in seen_sentences:
            unique_samples.append((words, tags))
            seen_sentences.add(sentence)

    print(f"  å»é‡å‰: {len(all_samples)} ä¸ª")
    print(f"  å»é‡å: {len(unique_samples)} ä¸ª")

    all_samples = unique_samples[:num_samples]

    # ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print("-" * 70)

    tag_counter = Counter()
    entity_counter = Counter()

    for words, tags in all_samples:
        for tag in tags:
            tag_counter[tag] += 1
            if tag.startswith('B-'):
                entity_counter[tag[2:]] += 1

    print(f"  æ€»æ ·æœ¬æ•°: {len(all_samples)}")
    print(f"  æ€»å­—ç¬¦æ•°: {sum(tag_counter.values())}")
    print(f"  æ€»å®ä½“æ•°: {sum(entity_counter.values())}")

    print(f"\n  æ ‡ç­¾åˆ†å¸ƒ:")
    for tag, count in sorted(tag_counter.items()):
        percentage = count / sum(tag_counter.values()) * 100
        print(f"    {tag:10s}: {count:5d} ({percentage:5.2f}%)")

    print(f"\n  å®ä½“ç±»å‹åˆ†å¸ƒ:")
    for entity_type, count in sorted(entity_counter.items()):
        percentage = count / sum(entity_counter.values()) * 100
        print(f"    {entity_type:5s}: {count:4d} ({percentage:5.2f}%)")

        # æ˜¾ç¤ºæ ·æœ¬ç¤ºä¾‹
    print(f"\nğŸ“– æ ·æœ¬ç¤ºä¾‹ (å‰5ä¸ª):")
    print("-" * 70)

    for i, (words, tags) in enumerate(all_samples[:5], 1):
        sentence = ''.join(words)
        print(f"\n  [{i}] {sentence}")
        print(f"      ", end="")

        j = 0
        while j < len(words):
            if tags[j].startswith('B-'):
                entity_type = tags[j][2:]
                entity_chars = [words[j]]
                k = j + 1
                while k < len(tags) and tags[k] == f'I-{entity_type}':
                    entity_chars.append(words[k])
                    k += 1
                entity_text = ''.join(entity_chars)
                print(f"[{entity_type}:{entity_text}]", end=" ")
                j = k
            else:
                print(words[j], end="")
                j += 1
        print()

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    import random
    random.shuffle(all_samples)

    split_idx = int(len(all_samples) * train_ratio)
    train_samples = all_samples[:split_idx]
    test_samples = all_samples[split_idx:]

    # ä¿å­˜æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜æ•°æ®...")
    print("-" * 70)

    os.makedirs(os.path.dirname(output_train) or '.', exist_ok=True)

    # ä¿å­˜è®­ç»ƒé›†
    with open(output_train, 'w', encoding='utf-8') as f:
        for words, tags in train_samples:
            for word, tag in zip(words, tags):
                f.write(f'{word} {tag}\n')
            f.write('\n')

    print(f"  âœ… è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬ â†’ {output_train}")

    # ä¿å­˜æµ‹è¯•é›†
    with open(output_test, 'w', encoding='utf-8') as f:
        for words, tags in test_samples:
            for word, tag in zip(words, tags):
                f.write(f'{word} {tag}\n')
            f.write('\n')

    print(f"  âœ… æµ‹è¯•é›†: {len(test_samples)} ä¸ªæ ·æœ¬ â†’ {output_test}")

    # æ€»ç»“
    print("\n" + "=" * 70)
    print("âœ… æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print("=" * 70)
    print(f"\n  ç”Ÿæˆæ ·æœ¬: {len(all_samples)}")
    print(f"  è®­ç»ƒé›†: {len(train_samples)}")
    print(f"  æµ‹è¯•é›†: {len(test_samples)}")
    print(f"  é”™è¯¯/è·³è¿‡: {error_count}")

    print("\nä¸‹ä¸€æ­¥:")
    print(f"  1. æŸ¥çœ‹æ•°æ®:")
    print(f"     head -50 {output_train}")
    print(f"\n  2. åˆå¹¶åˆ°ç°æœ‰æ•°æ®:")
    print(f"     cat data/train.txt {output_train} > data/train_merged.txt")
    print(f"\n  3. è®­ç»ƒæ¨¡å‹:")
    print(f"     python train.py")
    print("=" * 70)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ä½¿ç”¨ DeepSeek ç”Ÿæˆ NER æ•°æ®')
    parser.add_argument('--num_samples', type=int, default=200,
                        help='ç”Ÿæˆæ ·æœ¬æ•°é‡ (é»˜è®¤: 200)')
    parser.add_argument('--batch_size', type=int, default=50,
                        help='æ¯æ‰¹ç”Ÿæˆæ•°é‡ (é»˜è®¤: 50)')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                        help='è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤: 0.8)')
    parser.add_argument('--output_train', type=str, default='data/train_deepseek.txt',
                        help='è®­ç»ƒé›†è¾“å‡ºè·¯å¾„')
    parser.add_argument('--output_test', type=str, default='data/test_deepseek.txt',
                        help='æµ‹è¯•é›†è¾“å‡ºè·¯å¾„')

    args = parser.parse_args()

    generate_data_with_deepseek(
        num_samples=args.num_samples,
        batch_size=args.batch_size,
        output_train=args.output_train,
        output_test=args.output_test,
        train_ratio=args.train_ratio
    )


if __name__ == '__main__':
    main()